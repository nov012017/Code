{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('C:\\\\Analytics\\\\Personal\\\\Machine Learning\\\\Training\\\\R\\\\Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['times_pregnant' 'Plasma_glucose_concentration_2 hr' 'blood_pressure'\n",
      " ' Triceps_skin_fold_thickness ' ' Hr2_serum_insulin' 'BOI'\n",
      " ' Diabetes_pedigree_function' 'Age' 'Class']\n"
     ]
    }
   ],
   "source": [
    "# read the data in\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.651042\n",
      "1    0.348958\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# target variable % distribution\n",
    "print(df['Class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build a quick logistic regression model and check the accuracy\n",
    "\n",
    "X = df.iloc[:,:8] # independent variables\n",
    "y = df['Class'] # dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate a logistic regression model, and fit\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict class labels for the train set. The predict fuction converts probability values > .5 to 1 else 0\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate class probabilities\n",
    "# Notice that 2 elements will be returned in probs array,\n",
    "\n",
    "probs = model.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10927859,  0.89072141],\n",
       "       [ 0.78972605,  0.21027395],\n",
       "       [ 0.85929963,  0.14070037],\n",
       "       [ 0.39128836,  0.60871164],\n",
       "       [ 0.8255974 ,  0.1744026 ],\n",
       "       [ 0.92353765,  0.07646235],\n",
       "       [ 0.32291251,  0.67708749],\n",
       "       [ 0.25669134,  0.74330866],\n",
       "       [ 0.59214541,  0.40785459],\n",
       "       [ 0.62429282,  0.37570718],\n",
       "       [ 0.45786141,  0.54213859],\n",
       "       [ 0.1044592 ,  0.8955408 ],\n",
       "       [ 0.69141999,  0.30858001],\n",
       "       [ 0.7763338 ,  0.2236662 ],\n",
       "       [ 0.83010808,  0.16989192],\n",
       "       [ 0.79492177,  0.20507823],\n",
       "       [ 0.19582735,  0.80417265],\n",
       "       [ 0.93029869,  0.06970131],\n",
       "       [ 0.59223701,  0.40776299],\n",
       "       [ 0.68591115,  0.31408885],\n",
       "       [ 0.42864491,  0.57135509],\n",
       "       [ 0.64675796,  0.35324204],\n",
       "       [ 0.64309567,  0.35690433],\n",
       "       [ 0.9036953 ,  0.0963047 ],\n",
       "       [ 0.89736227,  0.10263773],\n",
       "       [ 0.62023166,  0.37976834],\n",
       "       [ 0.91298827,  0.08701173],\n",
       "       [ 0.17810709,  0.82189291],\n",
       "       [ 0.82052122,  0.17947878],\n",
       "       [ 0.79428368,  0.20571632],\n",
       "       [ 0.52622367,  0.47377633],\n",
       "       [ 0.7213916 ,  0.2786084 ],\n",
       "       [ 0.87219639,  0.12780361],\n",
       "       [ 0.52874841,  0.47125159],\n",
       "       [ 0.81705159,  0.18294841],\n",
       "       [ 0.3377263 ,  0.6622737 ],\n",
       "       [ 0.52367941,  0.47632059],\n",
       "       [ 0.86912701,  0.13087299],\n",
       "       [ 0.59807671,  0.40192329],\n",
       "       [ 0.30428295,  0.69571705],\n",
       "       [ 0.69586516,  0.30413484],\n",
       "       [ 0.7858872 ,  0.2141128 ],\n",
       "       [ 0.76514396,  0.23485604],\n",
       "       [ 0.23706703,  0.76293297],\n",
       "       [ 0.27601201,  0.72398799],\n",
       "       [ 0.96562809,  0.03437191],\n",
       "       [ 0.84117169,  0.15882831],\n",
       "       [ 0.71594937,  0.28405063],\n",
       "       [ 0.62322191,  0.37677809],\n",
       "       [ 0.69504479,  0.30495521],\n",
       "       [ 0.55749578,  0.44250422],\n",
       "       [ 0.73760552,  0.26239448],\n",
       "       [ 0.19011977,  0.80988023],\n",
       "       [ 0.53559803,  0.46440197],\n",
       "       [ 0.82079379,  0.17920621],\n",
       "       [ 0.98741573,  0.01258427],\n",
       "       [ 0.8875292 ,  0.1124708 ],\n",
       "       [ 0.58893465,  0.41106535],\n",
       "       [ 0.69324164,  0.30675836],\n",
       "       [ 0.74394832,  0.25605168],\n",
       "       [ 0.36730702,  0.63269298],\n",
       "       [ 0.53267682,  0.46732318],\n",
       "       [ 0.83234712,  0.16765288],\n",
       "       [ 0.2815969 ,  0.7184031 ],\n",
       "       [ 0.3726847 ,  0.6273153 ],\n",
       "       [ 0.1623601 ,  0.8376399 ],\n",
       "       [ 0.36029727,  0.63970273],\n",
       "       [ 0.80412302,  0.19587698],\n",
       "       [ 0.59169181,  0.40830819],\n",
       "       [ 0.83918858,  0.16081142],\n",
       "       [ 0.81967979,  0.18032021],\n",
       "       [ 0.44720975,  0.55279025],\n",
       "       [ 0.85648869,  0.14351131],\n",
       "       [ 0.14312213,  0.85687787],\n",
       "       [ 0.25419269,  0.74580731],\n",
       "       [ 0.67701231,  0.32298769],\n",
       "       [ 0.86568801,  0.13431199],\n",
       "       [ 0.40564398,  0.59435602],\n",
       "       [ 0.89073124,  0.10926876],\n",
       "       [ 0.77335809,  0.22664191],\n",
       "       [ 0.67077545,  0.32922455],\n",
       "       [ 0.59961518,  0.40038482],\n",
       "       [ 0.75562622,  0.24437378],\n",
       "       [ 0.92028342,  0.07971658],\n",
       "       [ 0.7401714 ,  0.2598286 ],\n",
       "       [ 0.78267166,  0.21732834],\n",
       "       [ 0.64410799,  0.35589201],\n",
       "       [ 0.54073229,  0.45926771],\n",
       "       [ 0.2090163 ,  0.7909837 ],\n",
       "       [ 0.78782814,  0.21217186],\n",
       "       [ 0.77239018,  0.22760982],\n",
       "       [ 0.78718278,  0.21281722],\n",
       "       [ 0.7010004 ,  0.2989996 ],\n",
       "       [ 0.91892982,  0.08107018],\n",
       "       [ 0.41596588,  0.58403412],\n",
       "       [ 0.73851515,  0.26148485],\n",
       "       [ 0.60895467,  0.39104533],\n",
       "       [ 0.52933733,  0.47066267],\n",
       "       [ 0.46773456,  0.53226544],\n",
       "       [ 0.72152427,  0.27847573],\n",
       "       [ 0.7352068 ,  0.2647932 ],\n",
       "       [ 0.83548029,  0.16451971],\n",
       "       [ 0.7807759 ,  0.2192241 ],\n",
       "       [ 0.91557142,  0.08442858],\n",
       "       [ 0.4390371 ,  0.5609629 ],\n",
       "       [ 0.63970547,  0.36029453],\n",
       "       [ 0.82152404,  0.17847596],\n",
       "       [ 0.66152342,  0.33847658],\n",
       "       [ 0.91349106,  0.08650894],\n",
       "       [ 0.27980132,  0.72019868],\n",
       "       [ 0.83402541,  0.16597459],\n",
       "       [ 0.66092381,  0.33907619],\n",
       "       [ 0.41726913,  0.58273087],\n",
       "       [ 0.65812051,  0.34187949],\n",
       "       [ 0.48187829,  0.51812171],\n",
       "       [ 0.43938641,  0.56061359],\n",
       "       [ 0.82481335,  0.17518665],\n",
       "       [ 0.33213996,  0.66786004],\n",
       "       [ 0.85898979,  0.14101021],\n",
       "       [ 0.34326459,  0.65673541],\n",
       "       [ 0.63145637,  0.36854363],\n",
       "       [ 0.69477932,  0.30522068],\n",
       "       [ 0.66796469,  0.33203531],\n",
       "       [ 0.53097845,  0.46902155],\n",
       "       [ 0.72619779,  0.27380221],\n",
       "       [ 0.90341794,  0.09658206],\n",
       "       [ 0.69050041,  0.30949959],\n",
       "       [ 0.63384367,  0.36615633],\n",
       "       [ 0.52694453,  0.47305547],\n",
       "       [ 0.62094463,  0.37905537],\n",
       "       [ 0.57428149,  0.42571851],\n",
       "       [ 0.83644517,  0.16355483],\n",
       "       [ 0.88883466,  0.11116534],\n",
       "       [ 0.30409721,  0.69590279],\n",
       "       [ 0.66746273,  0.33253727],\n",
       "       [ 0.59750695,  0.40249305],\n",
       "       [ 0.78503939,  0.21496061],\n",
       "       [ 0.60488064,  0.39511936],\n",
       "       [ 0.29693819,  0.70306181],\n",
       "       [ 0.73331476,  0.26668524],\n",
       "       [ 0.85254385,  0.14745615],\n",
       "       [ 0.47188582,  0.52811418],\n",
       "       [ 0.87047909,  0.12952091],\n",
       "       [ 0.88678133,  0.11321867],\n",
       "       [ 0.63146561,  0.36853439],\n",
       "       [ 0.85382236,  0.14617764],\n",
       "       [ 0.8650449 ,  0.1349551 ],\n",
       "       [ 0.8439462 ,  0.1560538 ],\n",
       "       [ 0.8237131 ,  0.1762869 ],\n",
       "       [ 0.77246386,  0.22753614],\n",
       "       [ 0.85320098,  0.14679902],\n",
       "       [ 0.46713938,  0.53286062],\n",
       "       [ 0.83453965,  0.16546035],\n",
       "       [ 0.78265946,  0.21734054],\n",
       "       [ 0.31600886,  0.68399114],\n",
       "       [ 0.80678239,  0.19321761],\n",
       "       [ 0.41330422,  0.58669578],\n",
       "       [ 0.79454205,  0.20545795],\n",
       "       [ 0.35967255,  0.64032745],\n",
       "       [ 0.06695361,  0.93304639],\n",
       "       [ 0.34618537,  0.65381463],\n",
       "       [ 0.28133327,  0.71866673],\n",
       "       [ 0.93770393,  0.06229607],\n",
       "       [ 0.69421699,  0.30578301],\n",
       "       [ 0.20394624,  0.79605376],\n",
       "       [ 0.54616554,  0.45383446],\n",
       "       [ 0.73014079,  0.26985921],\n",
       "       [ 0.81645646,  0.18354354],\n",
       "       [ 0.69130033,  0.30869967],\n",
       "       [ 0.86964599,  0.13035401],\n",
       "       [ 0.78377486,  0.21622514],\n",
       "       [ 0.74550723,  0.25449277],\n",
       "       [ 0.76520772,  0.23479228],\n",
       "       [ 0.7502134 ,  0.2497866 ],\n",
       "       [ 0.51310088,  0.48689912],\n",
       "       [ 0.81912057,  0.18087943],\n",
       "       [ 0.62723812,  0.37276188],\n",
       "       [ 0.8507758 ,  0.1492242 ],\n",
       "       [ 0.82318537,  0.17681463],\n",
       "       [ 0.87873537,  0.12126463],\n",
       "       [ 0.80753242,  0.19246758],\n",
       "       [ 0.30564435,  0.69435565],\n",
       "       [ 0.81512333,  0.18487667],\n",
       "       [ 0.12839787,  0.87160213],\n",
       "       [ 0.54130768,  0.45869232],\n",
       "       [ 0.94243983,  0.05756017],\n",
       "       [ 0.31014895,  0.68985105],\n",
       "       [ 0.70286151,  0.29713849],\n",
       "       [ 0.54124547,  0.45875453],\n",
       "       [ 0.81514038,  0.18485962],\n",
       "       [ 0.75843945,  0.24156055],\n",
       "       [ 0.85466539,  0.14533461],\n",
       "       [ 0.84488969,  0.15511031],\n",
       "       [ 0.71051218,  0.28948782],\n",
       "       [ 0.88075621,  0.11924379],\n",
       "       [ 0.27690216,  0.72309784],\n",
       "       [ 0.31386417,  0.68613583],\n",
       "       [ 0.49730795,  0.50269205],\n",
       "       [ 0.87822967,  0.12177033],\n",
       "       [ 0.69840754,  0.30159246],\n",
       "       [ 0.86961677,  0.13038323],\n",
       "       [ 0.79262011,  0.20737989],\n",
       "       [ 0.76343622,  0.23656378],\n",
       "       [ 0.60836888,  0.39163112],\n",
       "       [ 0.7176765 ,  0.2823235 ],\n",
       "       [ 0.80565245,  0.19434755],\n",
       "       [ 0.73316523,  0.26683477],\n",
       "       [ 0.78141224,  0.21858776],\n",
       "       [ 0.60816757,  0.39183243],\n",
       "       [ 0.81227026,  0.18772974],\n",
       "       [ 0.8614662 ,  0.1385338 ],\n",
       "       [ 0.86718409,  0.13281591],\n",
       "       [ 0.64292765,  0.35707235],\n",
       "       [ 0.65264333,  0.34735667],\n",
       "       [ 0.75141086,  0.24858914],\n",
       "       [ 0.66434613,  0.33565387],\n",
       "       [ 0.53062071,  0.46937929],\n",
       "       [ 0.66292099,  0.33707901],\n",
       "       [ 0.46941441,  0.53058559],\n",
       "       [ 0.75534171,  0.24465829],\n",
       "       [ 0.81583236,  0.18416764],\n",
       "       [ 0.90430254,  0.09569746],\n",
       "       [ 0.59232393,  0.40767607],\n",
       "       [ 0.41446952,  0.58553048],\n",
       "       [ 0.48318088,  0.51681912],\n",
       "       [ 0.34831506,  0.65168494],\n",
       "       [ 0.7254485 ,  0.2745515 ],\n",
       "       [ 0.70155369,  0.29844631],\n",
       "       [ 0.89273365,  0.10726635],\n",
       "       [ 0.80289605,  0.19710395],\n",
       "       [ 0.55771498,  0.44228502]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st element is probability for negative class,\n",
    "# 2nd element gives probability for positive class\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.89072141,  0.21027395,  0.14070037,  0.60871164,  0.1744026 ,\n",
       "        0.07646235,  0.67708749,  0.74330866,  0.40785459,  0.37570718,\n",
       "        0.54213859,  0.8955408 ,  0.30858001,  0.2236662 ,  0.16989192,\n",
       "        0.20507823,  0.80417265,  0.06970131,  0.40776299,  0.31408885,\n",
       "        0.57135509,  0.35324204,  0.35690433,  0.0963047 ,  0.10263773,\n",
       "        0.37976834,  0.08701173,  0.82189291,  0.17947878,  0.20571632,\n",
       "        0.47377633,  0.2786084 ,  0.12780361,  0.47125159,  0.18294841,\n",
       "        0.6622737 ,  0.47632059,  0.13087299,  0.40192329,  0.69571705,\n",
       "        0.30413484,  0.2141128 ,  0.23485604,  0.76293297,  0.72398799,\n",
       "        0.03437191,  0.15882831,  0.28405063,  0.37677809,  0.30495521,\n",
       "        0.44250422,  0.26239448,  0.80988023,  0.46440197,  0.17920621,\n",
       "        0.01258427,  0.1124708 ,  0.41106535,  0.30675836,  0.25605168,\n",
       "        0.63269298,  0.46732318,  0.16765288,  0.7184031 ,  0.6273153 ,\n",
       "        0.8376399 ,  0.63970273,  0.19587698,  0.40830819,  0.16081142,\n",
       "        0.18032021,  0.55279025,  0.14351131,  0.85687787,  0.74580731,\n",
       "        0.32298769,  0.13431199,  0.59435602,  0.10926876,  0.22664191,\n",
       "        0.32922455,  0.40038482,  0.24437378,  0.07971658,  0.2598286 ,\n",
       "        0.21732834,  0.35589201,  0.45926771,  0.7909837 ,  0.21217186,\n",
       "        0.22760982,  0.21281722,  0.2989996 ,  0.08107018,  0.58403412,\n",
       "        0.26148485,  0.39104533,  0.47066267,  0.53226544,  0.27847573,\n",
       "        0.2647932 ,  0.16451971,  0.2192241 ,  0.08442858,  0.5609629 ,\n",
       "        0.36029453,  0.17847596,  0.33847658,  0.08650894,  0.72019868,\n",
       "        0.16597459,  0.33907619,  0.58273087,  0.34187949,  0.51812171,\n",
       "        0.56061359,  0.17518665,  0.66786004,  0.14101021,  0.65673541,\n",
       "        0.36854363,  0.30522068,  0.33203531,  0.46902155,  0.27380221,\n",
       "        0.09658206,  0.30949959,  0.36615633,  0.47305547,  0.37905537,\n",
       "        0.42571851,  0.16355483,  0.11116534,  0.69590279,  0.33253727,\n",
       "        0.40249305,  0.21496061,  0.39511936,  0.70306181,  0.26668524,\n",
       "        0.14745615,  0.52811418,  0.12952091,  0.11321867,  0.36853439,\n",
       "        0.14617764,  0.1349551 ,  0.1560538 ,  0.1762869 ,  0.22753614,\n",
       "        0.14679902,  0.53286062,  0.16546035,  0.21734054,  0.68399114,\n",
       "        0.19321761,  0.58669578,  0.20545795,  0.64032745,  0.93304639,\n",
       "        0.65381463,  0.71866673,  0.06229607,  0.30578301,  0.79605376,\n",
       "        0.45383446,  0.26985921,  0.18354354,  0.30869967,  0.13035401,\n",
       "        0.21622514,  0.25449277,  0.23479228,  0.2497866 ,  0.48689912,\n",
       "        0.18087943,  0.37276188,  0.1492242 ,  0.17681463,  0.12126463,\n",
       "        0.19246758,  0.69435565,  0.18487667,  0.87160213,  0.45869232,\n",
       "        0.05756017,  0.68985105,  0.29713849,  0.45875453,  0.18485962,\n",
       "        0.24156055,  0.14533461,  0.15511031,  0.28948782,  0.11924379,\n",
       "        0.72309784,  0.68613583,  0.50269205,  0.12177033,  0.30159246,\n",
       "        0.13038323,  0.20737989,  0.23656378,  0.39163112,  0.2823235 ,\n",
       "        0.19434755,  0.26683477,  0.21858776,  0.39183243,  0.18772974,\n",
       "        0.1385338 ,  0.13281591,  0.35707235,  0.34735667,  0.24858914,\n",
       "        0.33565387,  0.46937929,  0.33707901,  0.53058559,  0.24465829,\n",
       "        0.18416764,  0.09569746,  0.40767607,  0.58553048,  0.51681912,\n",
       "        0.65168494,  0.2745515 ,  0.29844631,  0.10726635,  0.19710395,\n",
       "        0.44228502])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probability for positive class\n",
    "y_pred_prob = probs[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.78354978355\n"
     ]
    }
   ],
   "source": [
    "# generate evaluation metrics\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve : 0.838785\n"
     ]
    }
   ],
   "source": [
    "# extract false positive, true positive rate\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1-fpr       fpr       tpr\n",
      "0   1.000000  0.000000  0.013514\n",
      "1   1.000000  0.000000  0.054054\n",
      "2   0.993631  0.006369  0.054054\n",
      "3   0.993631  0.006369  0.067568\n",
      "4   0.987261  0.012739  0.067568\n",
      "5   0.987261  0.012739  0.094595\n",
      "6   0.980892  0.019108  0.094595\n",
      "7   0.980892  0.019108  0.243243\n",
      "8   0.974522  0.025478  0.243243\n",
      "9   0.974522  0.025478  0.310811\n",
      "10  0.968153  0.031847  0.310811\n",
      "11  0.968153  0.031847  0.351351\n",
      "12  0.961783  0.038217  0.351351\n",
      "13  0.961783  0.038217  0.418919\n",
      "14  0.949045  0.050955  0.418919\n",
      "15  0.949045  0.050955  0.432432\n",
      "16  0.942675  0.057325  0.432432\n",
      "17  0.942675  0.057325  0.459459\n",
      "18  0.936306  0.063694  0.459459\n",
      "19  0.936306  0.063694  0.472973\n",
      "20  0.929936  0.070064  0.472973\n",
      "21  0.929936  0.070064  0.513514\n",
      "22  0.917197  0.082803  0.513514\n",
      "23  0.917197  0.082803  0.527027\n",
      "24  0.898089  0.101911  0.527027\n",
      "25  0.898089  0.101911  0.540541\n",
      "26  0.872611  0.127389  0.540541\n",
      "27  0.872611  0.127389  0.554054\n",
      "28  0.847134  0.152866  0.554054\n",
      "29  0.847134  0.152866  0.594595\n",
      "..       ...       ...       ...\n",
      "43  0.745223  0.254777  0.756757\n",
      "44  0.738854  0.261146  0.756757\n",
      "45  0.738854  0.261146  0.783784\n",
      "46  0.732484  0.267516  0.783784\n",
      "47  0.732484  0.267516  0.797297\n",
      "48  0.675159  0.324841  0.797297\n",
      "49  0.675159  0.324841  0.810811\n",
      "50  0.662420  0.337580  0.810811\n",
      "51  0.662420  0.337580  0.824324\n",
      "52  0.643312  0.356688  0.824324\n",
      "53  0.643312  0.356688  0.851351\n",
      "54  0.636943  0.363057  0.851351\n",
      "55  0.636943  0.363057  0.878378\n",
      "56  0.630573  0.369427  0.878378\n",
      "57  0.630573  0.369427  0.905405\n",
      "58  0.579618  0.420382  0.905405\n",
      "59  0.579618  0.420382  0.918919\n",
      "60  0.573248  0.426752  0.918919\n",
      "61  0.573248  0.426752  0.932432\n",
      "62  0.535032  0.464968  0.932432\n",
      "63  0.535032  0.464968  0.945946\n",
      "64  0.528662  0.471338  0.945946\n",
      "65  0.528662  0.471338  0.959459\n",
      "66  0.401274  0.598726  0.959459\n",
      "67  0.401274  0.598726  0.972973\n",
      "68  0.382166  0.617834  0.972973\n",
      "69  0.382166  0.617834  0.986486\n",
      "70  0.197452  0.802548  0.986486\n",
      "71  0.197452  0.802548  1.000000\n",
      "72  0.000000  1.000000  1.000000\n",
      "\n",
      "[73 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "i = np.arange(len(tpr)) # index for df\n",
    "roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr,\n",
    "index = i),'1-fpr' : pd.Series(1-fpr, index = i)})\n",
    "\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEREpp6IgIiLlVBRERKScioKIiJT7/yRkyWSxO+iIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21d2698f278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot tpr vs 1-fpr\n",
    "#fig, ax = plt.subplots()\n",
    "plt.plot(roc['tpr'], label='tpr')\n",
    "#plt.plot(roc['1-fpr'], color = 'red', label='1-fpr')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('1-False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Which Error is Costly??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare Event or Imbalanced Dataset\n",
    "\n",
    "Providing an equal samples of positive and negative instances to the classification algorithm will result in an optimal result. Datasets that are highly skewed toward one or more classes have proven to be a challenge.\n",
    "\n",
    "Resampling is a common practice to address the imbalanced dataset issue.\n",
    "\n",
    "#Random under-sampling - Reduce majority class to match minority class count.\n",
    "\n",
    "#Random over-sampling - Increase minority class by randomly picking samples within minority class till counts of both class match.\n",
    "\n",
    "#Synthetic Minority Over-Sampling Technique (SMOTE) - Increase minority class by introducing synthetic examples through connecting all k (default = 5) minority class nearest neighbors using feature space similarity (Euclidean distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance\n",
    "A fundamental problem with supervised learning is the bias variance trade-off. Ideally a model should have two key characteristics.\n",
    "1. Sensitive enough to accurately capture the key patterns in the training dataset.\n",
    "2. It should be generalized enough to work well on any unseen datasets.\n",
    "Unfortunately, while trying to achieve the above-mentioned first point, there is an ample chance of over-fitting to noisy or unrepresentative training data points leading to a failure of generalizing the model. On the other hand, trying to generalize a model may result in failing to capture important regularities.\n",
    "\n",
    "### Bias\n",
    "If model accuracy is low on a training dataset as well as test dataset the model is said to be under-fitting or that the model has high bias. This means the model is not fitting the training dataset points well in regression or the decision boundary is not separating the classes well in classification; and two key reasons for bias are 1) not including the right features, and 2) not picking the correct order of polynomial degrees for model fitting.\n",
    "\n",
    "To solve an under-fitting issue or to reduced bias, try including more meaningful features and try to increase the model complexity by trying higher-order polynomial fittings.\n",
    "\n",
    "### Variance\n",
    "If a model is giving high accuracy on a training dataset, however on a test dataset the accuracy drops drastically, then the model is said to be over-fitting or a model that has high variance. The key reason for over-fitting is using higher-order polynomial degree (may not be required), which will fit decision boundary tools well to all data points including the noise of train dataset, instead of the underlying relationship. This will lead to a high accuracy (actual vs. predicted) in the train dataset and when applied to the test dataset, the prediction error will be high.\n",
    "To solve the over-fitting issue:\n",
    "\n",
    "#Try to reduce the number of features, that is, keep only the meaningful features.\n",
    "#Dimension reduction can eliminate noisy features, in turn, reducing the model variance.\n",
    "#Brining more data points to make training dataset large will also reduce variance.\n",
    "#Choosing right model parameters can help to reduce the bias and variance, for example.\n",
    "   #Using right regularization parameters can decrease variance in regression-based models.\n",
    "   #For a decision tree reducing the depth of the decision tree will reduce the variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation\n",
    "\n",
    "K-folds cross-validation splits the training dataset into k-folds without replacement, that is, any given data point will only be part of one of the subset, where k-1 folds are used for the model training and one fold is used for testing. The procedure is repeated k times so that we obtain k models and performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "X = df.iloc[:,:8].values # independent variables\n",
    "y = df['Class'].values # dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize Data\n",
    "from sklearn import preprocessing\n",
    "sc = preprocessing.StandardScaler()\n",
    "sc.fit(X)\n",
    "X = sc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a decision tree classifier\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "clf = tree.DecisionTreeClassifier(random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Fold AUC Scores:  [ 0.7037037   0.63888889  0.65420561  0.6635514   0.71028037]\n",
      "Train CV AUC Score:  0.674125995154\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using 10-fold cross-validation\n",
    "train_scores = cross_val_score(clf, X_train, y_train, \n",
    "                               scoring='accuracy', cv=5)\n",
    "test_scores = cross_val_score(clf, X_test, y_test, \n",
    "                              scoring='accuracy', cv=5)\n",
    "print(\"Train Fold AUC Scores: \", train_scores)\n",
    "print(\"Train CV AUC Score: \", train_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Fold AUC Scores:  [ 0.70212766  0.74468085  0.74468085  0.64444444  0.66666667]\n",
      "Test CV AUC Score:  0.700520094563\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest Fold AUC Scores: \", test_scores)\n",
    "print(\"Test CV AUC Score: \", test_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K-Fold Cross-Validation\n",
    "An extended cross-validation is the Stratified K-fold cross-validation, where the class proportions are preserved in each fold, leading to better\n",
    "bias and variance estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "Ensemble methods enable combining multiple model scores into a single score to create a robust generalized model.\n",
    "At a high level there are two types of ensemble methods.\n",
    "1. Combine multiple models of similar type\n",
    "     #### Bagging (Bootstrap aggregation)\n",
    "        Bootstrap aggregation (also known as bagging) was proposed by Leo Breiman in 1994, which is a model aggregation technique to reduce model variance. The training data is split into multiple samples with replacements called bootstrap samples. Bootstrap sample size will be the same as the original sample size, with 3/4th of the original values and replacement result in repetition of values\n",
    "     \n",
    "         Independent models on each of the bootstrap samples are built, and the average of the predictions for regression or majority vote for classification is used to create the final model.\n",
    " \n",
    "         Random Forest\n",
    "         \n",
    "     #### Boosting\n",
    "         The core concept of boosting is that rather than an independent individual hypothesis, combining hypotheses in a sequential order increases the accuracy. Essentially, boosting algorithms convert the weak learners into strong learners. Boosting algorithms are well designed to address the bias problems.\n",
    "         \n",
    "         At a high level the AdaBoosting (adaptive boosting) process can be divided into three steps.\n",
    "         1. Assign uniform weights for all data points W0(x) = 1 / N, where N is the total number of training data points.\n",
    "         2. At each iteration fit a classifier ym(xn) to the training data and update weights to minimize the weighted error function.\n",
    "         3. The final model.\n",
    "         \n",
    "         \n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
