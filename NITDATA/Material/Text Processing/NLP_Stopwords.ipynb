{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Anaconda Prompt\n",
    "Run the command\n",
    "pip install nltk\n",
    "or \n",
    "conda install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## After running the below line of a pop up will display. Click on the download button. \n",
    "## Close the pop up when all the installations are successfull\n",
    "\n",
    "nltk.download()\n",
    "\n",
    "\n",
    "## After running the below line of a pop up will display. Click on the download button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"If your web connection uses a proxy server, you should specify the proxy address as follows. In the case of an authenticating proxy, specify a username and password. If the proxy is set to None then this function will attempt to detect the system proxy.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences present in the Paragraph are  253\n"
     ]
    }
   ],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "\n",
    "print('Total Sentences present in the Paragraph are ',len(paragraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If your web connection uses a proxy server, you should specify the proxy address as follows.',\n",
       " 'In the case of an authenticating proxy, specify a username and password.',\n",
       " 'If the proxy is set to None then this function will attempt to detect the system proxy.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Total sentences available after sentence tokenize\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If your web connection uses a proxy server, you should specify the proxy address as follows.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the first sentence\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence present in the Paragraph is as below \n",
      "\n",
      " If your web connection uses a proxy server, you should specify the proxy address as follows.\n"
     ]
    }
   ],
   "source": [
    "print('First Sentence present in the Paragraph is as below \\n\\n',sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The below are the tokens from the first sentence \n",
      "\n",
      " ['If', 'your', 'web', 'connection', 'uses', 'a', 'proxy', 'server', ',', 'you', 'should', 'specify', 'the', 'proxy', 'address', 'as', 'follows', '.']\n"
     ]
    }
   ],
   "source": [
    "## Converting first sentence to tokens/words\n",
    "\n",
    "words=nltk.word_tokenize(sentences[0])\n",
    "print('The below are the tokens from the first sentence \\n\\n', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_words=[nltk.word_tokenize(i) for i in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "Stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"If your web connection uses a proxy server, you should specify the proxy address as follows. In the case of an authenticating proxy, specify a username and password. If the proxy is set to None then this function will attempt to detect the system proxy.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If your web connection uses a proxy server, you should specify the proxy address as follows.\n",
      "In the case of an authenticating proxy, specify a username and password.\n",
      "If the proxy is set to None then this function will attempt to detect the system proxy.\n"
     ]
    }
   ],
   "source": [
    "# Before applying stemming print the actual sentences\n",
    "for sent in range(len(sentences)):\n",
    "    print(sentences[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in range(len(sentences)):\n",
    "    #Converting sentences to words/tokens\n",
    "    tokens=nltk.word_tokenize(sentences[sent])\n",
    "    stem_tokens=[Stemmer.stem(token) for token in tokens]\n",
    "    sentences[sent]=' '.join(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After applying stemming print the updated sentences\n",
    "for sent in range(len(sentences)):\n",
    "    print(sentences[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"If your web connection uses a proxy server, you should specify the proxy address as follows. In the case of an authenticating proxy, specify a username and password. If the proxy is set to None then this function will attempt to detect the system proxy.\"\"\"\n",
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in range(len(sentences)):\n",
    "    tokens=nltk.word_tokenize(sentences[sent])\n",
    "    new_tokens=[]\n",
    "    for token in tokens:\n",
    "        stem_word=Stemmer.stem(token)\n",
    "        new_tokens.append(stem_word)\n",
    "    sentences[sent]=' '.join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If your web connect use a proxi server , you should specifi the proxi address as follow .\n",
      "In the case of an authent proxi , specifi a usernam and password .\n",
      "If the proxi is set to none then thi function will attempt to detect the system proxi .\n"
     ]
    }
   ],
   "source": [
    "# After applying stemming print the updated sentences\n",
    "for sent in range(len(sentences)):\n",
    "    print(sentences[sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing. Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960's. In this tutorial you will learn about Stemming and Lemmatization in a practical approach covering the background, some famous algorithms, applications of Stemming and Lemmatization, and how to stem and lemmatize words, sentences and documents using the Python nltk package which is the Natural Language Tool Kit package provided by Python for Natural Language Processing tasks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"If your web connection uses a proxy server, you should specify the proxy address as follows. In the case of an authenticating proxy, specify a username and password. If the proxy is set to None then this function will attempt to detect the system proxy.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "Lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing.\n",
      "Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960's.\n",
      "In this tutorial you will learn about Stemming and Lemmatization in a practical approach covering the background, some famous algorithms, applications of Stemming and Lemmatization, and how to stem and lemmatize words, sentences and documents using the Python nltk package which is the Natural Language Tool Kit package provided by Python for Natural Language Processing tasks.\n"
     ]
    }
   ],
   "source": [
    "# Before applying stemming print the actual sentences\n",
    "for sent in range(len(sentences)):\n",
    "    print(sentences[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in range(len(sentences)):\n",
    "    #Converting sentences to words/tokens\n",
    "    tokens=nltk.word_tokenize(sentences[sent])\n",
    "    lemma_tokens=[Lemmatizer.lemmatize(token) for token in tokens]\n",
    "    sentences[sent]=' '.join(lemma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming and Lemmatization are Text Normalization ( or sometimes called Word Normalization ) technique in the field of Natural Language Processing that are used to prepare text , word , and document for further processing .\n",
      "Stemming and Lemmatization have been studied , and algorithm have been developed in Computer Science since the 1960 's .\n",
      "In this tutorial you will learn about Stemming and Lemmatization in a practical approach covering the background , some famous algorithm , application of Stemming and Lemmatization , and how to stem and lemmatize word , sentence and document using the Python nltk package which is the Natural Language Tool Kit package provided by Python for Natural Language Processing task .\n"
     ]
    }
   ],
   "source": [
    "# After applying stemming print the updated sentences\n",
    "for sent in range(len(sentences)):\n",
    "    print(sentences[sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing. Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960's. In this tutorial you will learn about Stemming and Lemmatization in a practical approach covering the background, some famous algorithms, applications of Stemming and Lemmatization, and how to stem and lemmatize words, sentences and documents using the Python nltk package which is the Natural Language Tool Kit package provided by Python for Natural Language Processing tasks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stop words available 179\n",
      "Stowords are \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "## Stopwords\n",
    "Stopwords=stopwords.words('english')\n",
    "print('Total number of stop words available', len(Stopwords))\n",
    "print('Stowords are \\n',Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing.\n",
      "Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960's.\n",
      "In this tutorial you will learn about Stemming and Lemmatization in a practical approach covering the background, some famous algorithms, applications of Stemming and Lemmatization, and how to stem and lemmatize words, sentences and documents using the Python nltk package which is the Natural Language Tool Kit package provided by Python for Natural Language Processing tasks.\n"
     ]
    }
   ],
   "source": [
    "# Before applying stemming print the actual sentences\n",
    "for sent in range(len(sentences)):\n",
    "    print(sentences[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in range(len(sentences)):\n",
    "    #Converting sentences to words/tokens\n",
    "    tokens=nltk.word_tokenize(sentences[sent])\n",
    "    lemma_tokens=[token for token in tokens if token not in Stopwords]\n",
    "    sentences[sent]=' '.join(lemma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming Lemmatization Text Normalization ( sometimes called Word Normalization ) techniques field Natural Language Processing used prepare text , words , documents processing .\n",
      "Stemming Lemmatization studied , algorithms developed Computer Science since 1960 's .\n",
      "In tutorial learn Stemming Lemmatization practical approach covering background , famous algorithms , applications Stemming Lemmatization , stem lemmatize words , sentences documents using Python nltk package Natural Language Tool Kit package provided Python Natural Language Processing tasks .\n"
     ]
    }
   ],
   "source": [
    "# After applying stemming print the updated sentences\n",
    "for sent in range(len(sentences)):\n",
    "    print(sentences[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing. Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960's. In this tutorial you will learn about Stemming and Lemmatization in a practical approach covering the background, some famous algorithms, applications of Stemming and Lemmatization, and how to stem and lemmatize words, sentences and documents using the Python nltk package which is the Natural Language Tool Kit package provided by Python for Natural Language Processing tasks.\"\"\"\n",
    "\n",
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in range(len(sentences)):\n",
    "    tokens=nltk.word_tokenize(sentences[sent])\n",
    "    new_tokens=[]\n",
    "    for token in tokens:\n",
    "        if token not in Stopwords:\n",
    "            stem_word=Stemmer.stem(token)\n",
    "            new_tokens.append(stem_word)\n",
    "    sentences[sent]=' '.join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem lemmat text normal ( sometim call word normal ) techniqu field natur languag process use prepar text , word , document process .\n",
      "stem lemmat studi , algorithm develop comput scienc sinc 1960 's .\n",
      "In tutori learn stem lemmat practic approach cover background , famou algorithm , applic stem lemmat , stem lemmat word , sentenc document use python nltk packag natur languag tool kit packag provid python natur languag process task .\n"
     ]
    }
   ],
   "source": [
    "# After applying stemming print the updated sentences\n",
    "for sent in range(len(sentences)):\n",
    "    print(sentences[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
