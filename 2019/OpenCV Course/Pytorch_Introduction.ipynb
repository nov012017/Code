{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">Introduction to Pytorch</font>\n",
    "This notebook will provide a brief overview of PyTorch and how it is similar to Numpy. The goal of this notebook is to understand the basic data structures required to build Deep Learning models and train them.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">Why do we need PyTorch when we already have Numpy?</font>\n",
    "Deep Learning involves performing similar operations like convolutions and multiplications repetitively. Thus there is a need to run the code on GPUs which can parallelize these operations over multiple cores - these devices are perfectly suitable for doing massive matrix operations and are much faster than CPUs.\n",
    "\n",
    "On the diagram below you can find iteration times for several well-known architectures of neural networks running on different hardware. Please note, that the vertical axis has a logarithmic scale. Data for this chart was from taken [here](https://github.com/jcjohnson/cnn-benchmarks).\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w1_forward-backward-time-scaled.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While NumPy with its various backends suits perfectly for doing calculus on CPU, it lacks the GPU computations support. And this is the first reason why we need Pytorch.\n",
    "\n",
    "The other reason is that Numpy is a genral purpose library. PyTorch ( or any other modern deep learning library ) has optimized code for many deep learning specific operations (e.g. Gradient calculations ) which are not present in Numpy.\n",
    "\n",
    "So, let's take a look at what are the data structures of Pytorch and how it provides us its cool features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">Tensor - Pytorch's core data structure</font>\n",
    "\n",
    "As we've already seen, in python we can create lists, lists of lists, lists of lists of lists and so on. In NumPy there is a `numpy.ndarray` which represents `n`-dimensional array, e.g. 3-dimensional if we want to find an analog for a list of lists of lists. In math, there is a special name for the generalization of vectors and matrices to a higher dimensional space - a tensor.\n",
    "\n",
    "Tensor is an entity with a defined number of dimensions called an order (rank).\n",
    "\n",
    "**Scalars** can be considered as a rank-0-tensor. Let's denote scalar value as $x \\in \\mathbb{R}$, where $\\mathbb{R}$ is a set of real numbers.\n",
    "\n",
    "**Vectors** can be introduced as a rank-1-tensor. Vectors belong to linear space (vector space), which is, in simple terms, a set of possible vectors of a specific length. A vector consisting of real-valued scalars ($x \\in \\mathbb{R}$) can be defined as ($y \\in \\mathbb{R}^n$), where $y$ - vector value and $\\mathbb{R}^n$ - $n$-dimensional real-number vector space. $y_i$ - $i_{th}$ vector element (scalar):\n",
    "\n",
    "$$\n",
    " y = \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}\n",
    "         \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Matrices** can be considered as a rank-2-tensor. A matrix of size $m \\times n$, where $m, n \\in \\mathbb{N}$ (rows and columns number accordingly) consisting of real-valued scalars can be denoted as $A \\in \\mathbb{R}^{m \\times n}$, where $\\mathbb{R}^{m \\times n}$ is a real-valued $m \\times n$-dimensional vector space:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "    x_{11}       & x_{12} & x_{13} & \\dots & x_{1n} \\\\\n",
    "    x_{21}       & x_{22} & x_{23} & \\dots & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1}       & x_{m2} & x_{m3} & \\dots & x_{mn}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the picture below you can find different tensor dimensions, where the fourth and the fifth cases are a vector of cubes and a matrix of cubes accordingly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: 5px solid red\">\n",
    "    <tr><th><center>Rank-1-tensor</center></th><th><center>Rank-2-tensor</center></th><th><center>Rank-3-tensor</center></th><th><center>Rank-4-tensor</center></th><th><center>Rank-5-tensor</center></th></tr>\n",
    "        <th><center><img align=\"middle\" src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w1_1D_tensor_.jpg\" width=\"30\" height=\"30\"></center></th>\n",
    "        <th><center><img align=\"middle\" src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w1_2D_tensor.jpg\" width=\"130\" height=\"130\"></center></th>\n",
    "        <th><center><img align=\"middle\" src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w1_3D_tensor.jpg\" width=\"145\" height=\"145\"></center></th>\n",
    "        <th><center><img align=\"middle\" src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w1_4D_tensor_.jpg\" width=\"120\" height=\"120\"></center></th>\n",
    "        <th><center><img align=\"middle\" src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w1_matrix_of_cubes.jpg\" width=\"240\" height=\"240\"></center></th>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Tensor basics</font>\n",
    "Let's import the torch module first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Tensor Creation </font>\n",
    "Let's view examples of matrices and tensors generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-dimensional (rank-2) tensor of zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random rank-4 tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.7372, 0.9042],\n",
       "          [0.8952, 0.0870]],\n",
       "\n",
       "         [[0.2387, 0.3206],\n",
       "          [0.2956, 0.9276]]],\n",
       "\n",
       "\n",
       "        [[[0.8090, 0.4686],\n",
       "          [0.9939, 0.0549]],\n",
       "\n",
       "         [[0.0936, 0.6017],\n",
       "          [0.0594, 0.5533]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 2, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sure, there are many more ways to create tensor using some restrictions on values it should contain - for the full reference, please follow the [official docs](https://pytorch.org/docs/stable/torch.html#creation-ops)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Python / NumPy / Pytorch interoperability</font>\n",
    "You can create tensors from python lists as well as numpy arrays. You can also convert torch tensors to numpy arrays. So, the interoperability between torch and numpy is pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List:    [1, 2]\n",
      "Array:   [1 2]\n",
      "Tensor:  tensor([1, 2])\n",
      "Tensor:  tensor([1, 2], dtype=torch.int32)\n",
      "Tensor:  tensor([1, 2], dtype=torch.int32)\n",
      "Array:   [1 2]\n"
     ]
    }
   ],
   "source": [
    "# Simple Python list\n",
    "python_list = [1, 2]\n",
    "\n",
    "# Create a numpy array from python list\n",
    "numpy_array = np.array(python_list)\n",
    "\n",
    "# Create a torch Tensor from python list\n",
    "tensor_from_list = torch.tensor(python_list)\n",
    "\n",
    "# Create a torch Tensor from Numpy array\n",
    "tensor_from_array = torch.tensor(numpy_array)\n",
    "\n",
    "# Another way to create a torch Tensor from Numpy array (Share same storage)\n",
    "tensor_from_array_v2 = torch.from_numpy(numpy_array)\n",
    "\n",
    "# Convert torch tensor to numpy array\n",
    "array_from_tensor = tensor_from_array.numpy()\n",
    "\n",
    "print('List:   ', python_list)\n",
    "print('Array:  ', numpy_array)\n",
    "print('Tensor: ', tensor_from_list)\n",
    "print('Tensor: ', tensor_from_array)\n",
    "print('Tensor: ', tensor_from_array_v2)\n",
    "print('Array:  ', array_from_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differnce between `torch.Tensor` and `torch.from_numpy`\n",
    "\n",
    "Pytorch aims to be an effective library for computations. What does it mean? It means that pytorch avoids memory copying if it can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array:   [10  2]\n",
      "Tensor:  tensor([1, 2])\n",
      "Tensor:  tensor([10,  2])\n"
     ]
    }
   ],
   "source": [
    "numpy_array[0] = 10\n",
    "\n",
    "print('Array:  ', numpy_array)\n",
    "print('Tensor: ', tensor_from_array)\n",
    "print('Tensor: ', tensor_from_array_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have two different ways to create tensor from its NumPy counterpart - one copies memory and another one shares the same underlying storage. It also works in the opposite way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:  tensor([1, 2], dtype=torch.int32)\n",
      "Array:  [1 2]\n",
      "Tensor:  tensor([11,  2], dtype=torch.int32)\n",
      "Array:  [11  2]\n"
     ]
    }
   ],
   "source": [
    "array_from_tensor = tensor_from_array.numpy()\n",
    "print('Tensor: ', tensor_from_array)\n",
    "print('Array: ', array_from_tensor)\n",
    "\n",
    "tensor_from_array[0] = 11\n",
    "print('Tensor: ', tensor_from_array)\n",
    "print('Array: ', array_from_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Data types</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic data type for all Deep Learning-related operations is float, but sometimes you may need something else. Pytorch supports different numbers types for its tensors the same way as NumPy does it - by specifying the data type on tensor creation or via casting. The full list of supported data types can be found [here](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor with default type:  tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "Tensor with 16-bit float:  tensor([[0., 0.],\n",
      "        [0., 0.]], dtype=torch.float16)\n",
      "Tensor with integers:  tensor([[0, 0],\n",
      "        [0, 0]], dtype=torch.int16)\n",
      "Tensor with boolean data:  tensor([[False, False],\n",
      "        [False, False]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros(2, 2)\n",
    "print('Tensor with default type: ', tensor)\n",
    "tensor = torch.zeros(2, 2, dtype=torch.float16)\n",
    "print('Tensor with 16-bit float: ', tensor)\n",
    "tensor = torch.zeros(2, 2, dtype=torch.int16)\n",
    "print('Tensor with integers: ', tensor)\n",
    "tensor = torch.zeros(2, 2, dtype=torch.bool)\n",
    "print('Tensor with boolean data: ', tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Indexing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor provides access to its elements via the same `[]` operation as a regular python list or NumPy array. However, as you may recall from NumPy usage, the full power of math libraries is accessible only via vectorized operations, i.e. operations without explicit looping over all vector elements in python and using implicit optimized loops in C/C++/CUDA/Fortran/etc. available via special functions calls. Pytorch employs the same paradigm and provides a wide range of vectorized operations. Let's take a look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining a list of tensors together with `torch.cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0., 0., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(3, 2)\n",
    "b = torch.ones(3, 2)\n",
    "print(torch.cat((a, b), dim=0))\n",
    "print(torch.cat((a, b), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing with another tensor/array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "[False False False False False False  True  True  True  True]\n",
      "tensor([6, 7, 8, 9])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 0, 1, 2])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(start=0, end=10)\n",
    "indices = np.arange(0, 10) > 5\n",
    "print(a)\n",
    "print(indices)\n",
    "print(a[indices])\n",
    "\n",
    "indices = torch.arange(start=0, end=10) % 7\n",
    "print(indices)\n",
    "print(a[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we do if we have, say, rank-2 tensor and want to select only some rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1631, 0.2767, 0.5761],\n",
      "        [0.4693, 0.5638, 0.0687],\n",
      "        [0.9817, 0.1471, 0.7682],\n",
      "        [0.8303, 0.9732, 0.9074],\n",
      "        [0.0397, 0.0777, 0.1734]])\n",
      "tensor([[0.1631, 0.2767, 0.5761],\n",
      "        [0.9817, 0.1471, 0.7682],\n",
      "        [0.0397, 0.0777, 0.1734]])\n",
      "tensor([[0.9817, 0.1471, 0.7682],\n",
      "        [0.8303, 0.9732, 0.9074],\n",
      "        [0.0397, 0.0777, 0.1734]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand((5, 3))\n",
    "rows = torch.tensor([0, 2, 4])\n",
    "tensor[rows]\n",
    "print(tensor)\n",
    "print(tensor[rows])\n",
    "print(tensor[[2,3,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Tensor Shapes</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping a tensor is a frequently used operation. We can change the shape of a tensor without the memory copying overhead. There are two methods for that: `reshape` and `view`.\n",
    "\n",
    "The difference is the following:\n",
    "- view tries to return the tensor, and it shares the same memory with the original tensor. In case, if it cannot reuse the same memory due to [some reasons](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view), it just fails.\n",
    "- reshape always returns the tensor with the desired shape and tries to reuse the memory. If it cannot, it creates a copy.\n",
    "\n",
    "Let's see with the help of an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer to data:  2307421181632\n",
      "Shape:  torch.Size([2, 3, 4])\n",
      "Reshaped tensor - pointer to data 2307421181632\n",
      "Reshaped tensor shape  torch.Size([24])\n",
      "Viewed tensor - pointer to data 2307421181632\n",
      "Viewed tensor shape  torch.Size([3, 2, 4])\n",
      "Original stride:  (12, 4, 1)\n",
      "Reshaped stride:  (1,)\n",
      "Viewed stride:  (8, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(2, 3, 4)\n",
    "print('Pointer to data: ', tensor.data_ptr())\n",
    "print('Shape: ', tensor.shape)\n",
    "\n",
    "reshaped = tensor.reshape(24)\n",
    "\n",
    "view = tensor.view(3, 2, 4)\n",
    "print('Reshaped tensor - pointer to data', reshaped.data_ptr())\n",
    "print('Reshaped tensor shape ', reshaped.shape)\n",
    "\n",
    "print('Viewed tensor - pointer to data', view.data_ptr())\n",
    "print('Viewed tensor shape ', view.shape)\n",
    "\n",
    "assert tensor.data_ptr() == view.data_ptr()\n",
    "\n",
    "assert np.all(np.equal(tensor.numpy().flat, reshaped.numpy().flat))\n",
    "\n",
    "print('Original stride: ', tensor.stride())\n",
    "print('Reshaped stride: ', reshaped.stride())\n",
    "print('Viewed stride: ', view.stride())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7826, 0.4752, 0.8983, 0.3013],\n",
       "         [0.1576, 0.9269, 0.4364, 0.4397],\n",
       "         [0.5694, 0.2159, 0.7275, 0.1419]],\n",
       "\n",
       "        [[0.1069, 0.2924, 0.6724, 0.2303],\n",
       "         [0.8074, 0.0958, 0.0463, 0.8753],\n",
       "         [0.8479, 0.5649, 0.9213, 0.9610]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic rule about reshaping the tensor is definitely that you cannot change the total number of elements in it, so the product of all tensor's dimensions should always be the same. It gives us the ability to avoid specifying one dimension when reshaping the tensor - Pytorch can calculate it for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 4])\n",
      "torch.Size([3, 2, 4])\n",
      "torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.reshape(3, 2, 4).shape)\n",
    "print(tensor.reshape(3, 2, -1).shape)\n",
    "print(tensor.reshape(3, -1, 4).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative ways to view tensors** - `expand` or `expand_as`. \n",
    "\n",
    "- `expand` - requires the desired shape as an input\n",
    "- `expand_as` - uses the shape of another tensor. \n",
    "\n",
    "These operaitions \"repeat\" tensor's values along the specified axes without actual copying the data. \n",
    "\n",
    "As the documentation says, expand\n",
    "> returns a new view of the self tensor with singleton dimensions expanded to a larger size.\n",
    "Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1.\n",
    "\n",
    "**Use case:**\n",
    "\n",
    "- index multi-channel tensor with single-channel mask - imagine a color image with 3 channels (R, G and B) and binary mask for the area of interest on that image. We cannot index the image with this kind of mask directly since the dimensions are different, but we can use `expand_as` operation to create a view of the mask that has the same dimensions as the image we want to apply it to, but has not copied the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAMeUlEQVR4nO3dT6xU533G8e9TiFk0XkAdEAVa04guyIakV2wSVe6itesNzsIVWVRUtUQWtpQsKhXHi3iZVkq6SySiWGaRmiIlkVn0n2tFclexwXIdMCW+iV1zAwJFVIrbhVPwr4t7UCZkLjP3zv/3fj/S0cy89z1z3nfubx7OHM6cm6pCktSW35j1ACRJ42e4S1KDDHdJapDhLkkNMtwlqUGGuyQ1aGLhnuSRJJeTLCc5MantSNNkXWtRZBLnuSfZAvwI+GNgBXgN+FxVvTX2jUlTYl1rkUxqz/0wsFxVP6mqXwCngSMT2pY0Lda1Fsakwn0PcKXn8UrXJi0y61oLY+uEnjd92n7l+E+S48Dx7uEfTGgc0h0/q6qPjfgcA+sarG1NV1X1q8uJhfsKsK/n8V7g6l0DOgmcBEjiBW40af81hucYWNdgbWs+TOqwzGvAgST7k9wHHAXOTmhb0rRY11oYE9lzr6pbSZ4C/gXYAjxXVRcnsS1pWqxrLZKJnAq57kH40VWTd76qlqa9UWtbk7bWMXe/oSpJDTLcJalBhrskNchwl6QGGe6S1KBJfYlpcjz3QHf0PUdgcc3DmWuaD8noxe2euyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0a6XruSd4F3gduA7eqainJDuAfgAeBd4E/q6r/Hm2Y0nRZ21p049hz/6OqOlRVS93jE8DLVXUAeLl7LC0ia1sLaxKHZY4Ap7r7p4DHJrANaRasbS2MUcO9gH9Ncj7J8a5tV1VdA+hud464DWkWrG0ttFH/huqnq+pqkp3AS0n+c9gVuzfM8YEdpdmwtrXQRtpzr6qr3e0N4HvAYeB6kt0A3e2NNdY9WVVLPcczpblhbWvRbTjck/xmkvvv3Af+BLgAnAWOdd2OAS+OOkhpmqxttWCUwzK7gO8lufM8f19V/5zkNeBMkieA94DHRx+mNFXWthZeqmrWYyDJ8IOY/XA1L7Ku3udncZhkPbU9D+9FzYdux2IoVdW3s99QlaQGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQwHBP8lySG0ku9LTtSPJSkre72+09P3s6yXKSy0kentTApVFZ22rZMHvuzwOP3NV2Ani5qg4AL3ePSXIQOAp8olvn60m2jG200ng9j7WtRg0M96p6Bbh5V/MR4FR3/xTwWE/76ar6oKreAZaBw2MaqzRW1rZattFj7ruq6hpAd7uza98DXOnpt9K1/Zokx5OcS3Jug2OQJsHaVhO2jvn50qet+nWsqpPASYAkfftIc8Ta1kLZ6J779SS7AbrbG137CrCvp99e4OrGhydNnbWtJmw03M8Cx7r7x4AXe9qPJtmWZD9wAHh1tCFKU2VtqwkDD8skeQF4CHggyQrwZeArwJkkTwDvAY8DVNXFJGeAt4BbwJNVdXtCY5dGYm2rZama/SHBdR2XnP1wNS/6HQVf2/mqWprQSNa0ntqeh/ei5kMyfHFXVd/OfkNVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNGhjuSZ5LciPJhZ62Z5P8NMkb3fJoz8+eTrKc5HKShyc1cGlU1rZaNsye+/PAI33a/66qDnXLPwIkOQgcBT7RrfP1JFvGNVhpzJ7H2lajBoZ7Vb0C3Bzy+Y4Ap6vqg6p6B1gGDo8wPmlirG21bJRj7k8lebP7aLu9a9sDXOnps9K1SYvE2tbC22i4fwP4OHAIuAZ8tWtPn77V7wmSHE9yLsm5DY5BmgRrW03YULhX1fWqul1VHwLf5JcfT1eAfT1d9wJX13iOk1W1VFVLGxmDNAnWtlqxoXBPsrvn4WeBO2cbnAWOJtmWZD9wAHh1tCFK02NtqxVbB3VI8gLwEPBAkhXgy8BDSQ6x+rH0XeDzAFV1MckZ4C3gFvBkVd2ezNCl0Vjbalmq+h42nO4gkuEHMfvhal70Owq+tvOzOEyyntqeh/ei5kMyfHFXVd/OfkNVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUEDwz3JviTfT3IpycUkX+jadyR5Kcnb3e32nnWeTrKc5HKShyc5AWmjrG21LFV17w7JbmB3Vb2e5H7gPPAY8BfAzar6SpITwPaq+uskB4EXgMPAbwP/Bvx+Vd2+xzbuPYhew/dU67Ku3ueraulXVp+z2h70XtTmkQxf3FXVt/PAPfequlZVr3f33wcuAXuAI8CprtspVt8UdO2nq+qDqnoHWGb1zSDNFWtbLVvXMfckDwKfBH4A7Kqqa7D6JgF2dt32AFd6Vlvp2qS5ZW2rNVuH7Zjko8B3gC9W1c/v8bGh3w9+7fNmkuPA8WG3L02Kta0WDbXnnuQjrBb/t6vqu13z9e6Y5Z1jlze69hVgX8/qe4Grdz9nVZ2sqqW7j4NK02Rtq1XDnC0T4FvApar6Ws+PzgLHuvvHgBd72o8m2ZZkP3AAeHV8Q5bGw9pWy4Y5W+YzwL8DPwQ+7Jq/xOqxyTPA7wDvAY9X1c1unWeAvwRusfpR958GbMOzZbR+o58tM1e17dkyumMcZ8sMDPdpMNy1ISOG+zQY7tqIqZwKKUlaPIa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBW2c9gHVb31/fkRbGev76jjSIe+6S1CDDXZIaZLhLUoMGhnuSfUm+n+RSkotJvtC1P5vkp0ne6JZHe9Z5OslykstJHp7kBKSNsrbVtKq65wLsBj7V3b8f+BFwEHgW+Ks+/Q8C/wFsA/YDPwa2DNhGubhMeDlnbbu0uKxVewP33KvqWlW93t1/H7gE7LnHKkeA01X1QVW9AywDhwdtR5o2a1stW9cx9yQPAp8EftA1PZXkzSTPJdnete0BrvSstsK93zDSzFnbas3Q4Z7ko8B3gC9W1c+BbwAfBw4B14Cv3unaZ/Xq83zHk5xLcm7do5bGyNpWi4YK9yQfYbX4v11V3wWoqutVdbuqPgS+yS8/nq4A+3pW3wtcvfs5q+pkVS1V1dIoE5BGYW2rVcOcLRPgW8ClqvpaT/vunm6fBS50988CR5NsS7IfOAC8Or4hS+Nhbatlw1x+4NPAnwM/TPJG1/Yl4HNJDrH6sfRd4PMAVXUxyRngLeAW8GRV3R6wjZ8B/9vdbgYPsHnmCvMx39/t0zaN2v4f4PLow18Y8/C7npZ5mGu/ugYg3elaM5fk3Gb5GLuZ5gqbb769NtvcN9N8532ufkNVkhpkuEtSg+Yp3E/OegBTtJnmCptvvr0229w303zneq5zc8xdkjQ+87TnLkkak5mHe5JHuivsLSc5MevxjEP3lfUbSS70tO1I8lKSt7vb7T0/W9grDd7jyopNznc9Wqtt63rB5jvoqpCTXIAtrF5Z7/eA+1i94t7BWY5pTPP6Q+BTwIWetr8FTnT3TwB/Uxu80uA8Lax9ZcUm57uO16W52rauF6uuZ73nfhhYrqqfVNUvgNOsXnlvoVXVK8DNu5qPAKe6+6eAx3raF/ZKg7X2lRWbnO86NFfb1vVi1fWsw30zXWVvV1Vdg9XCAXZ27c28BnddWbH5+Q6wWebZ/O95Uet61uE+1FX2GtfEa9Dnyoprdu3TtnDzHcJmmedampj/Itf1rMN9qKvsNeL6nQtSdbc3uvaFfw36XVmRhuc7pM0yz2Z/z4te17MO99eAA0n2J7kPOMrqlfdadBY41t0/BrzY076wVxpc68qKNDrfddgstd3k77mJup6D/5V+lNX/if4x8MysxzOmOb3A6h95+D9W/0V/Avgt4GXg7e52R0//Z7r5Xwb+dNbjX+dcP8Pqx883gTe65dFW57vO16ap2rauF6uu/YaqJDVo1odlJEkTYLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSg/wf+E8GqMQxmCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create a black image\n",
    "image = torch.zeros(size=(3, 256, 256), dtype=torch.int)\n",
    "\n",
    "# Leave the borders and make the rest of the image Green\n",
    "image[1, 18:256 - 18, 18:256 - 18] = 255\n",
    "\n",
    "# Create a mask of the same size\n",
    "mask = torch.zeros(size=(256, 256), dtype=torch.bool)\n",
    "\n",
    "# Assuming the green region in the original image is the Region of interest, change the mask to white for that area\n",
    "mask[18:256 - 18, 18:256 - 18] = 1\n",
    "\n",
    "# Create a view of the mask with the same dimensions as the original image\n",
    "mask_expanded = mask.expand_as(image)\n",
    "print(mask_expanded.shape)\n",
    "\n",
    "mask_np = mask_expanded.numpy().transpose(1, 2, 0) * 255\n",
    "image_np = image.numpy().transpose(1, 2, 0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(image_np)\n",
    "ax[1].imshow(mask_np)\n",
    "plt.show()\n",
    "\n",
    "image[0, mask] += 128\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(image_np)\n",
    "ax[1].imshow(mask_np)\n",
    "plt.show()\n",
    "\n",
    "image[mask_expanded] += 128\n",
    "image.clamp_(0, 255)\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(image_np)\n",
    "ax[1].imshow(mask_np)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, one can also find a couple of useful tricks:\n",
    "- `clamp` method and function is a Pytorch's analogue of NumPy's `clip` function\n",
    "- many operations on tensors have in-place form, that does not return modified data, but change values in the tensor. The in-place version of the operation has trailing underscore according to Pytorch's naming convention - in the example above it is `clamp_`\n",
    "- tensors have the same indexing as Numpy's arrays - one can use `:`-separated range, negative indexes and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">Images and their representations</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's discuss images, their representations and how different Python libraries work with them.\n",
    "\n",
    "Probably, the most well-known library for image loading and simple processing is [Pillow](https://pillow.readthedocs.io/en/stable/). Another common alternative, that supports lots of image (and not only image) formats is [imageio](https://imageio.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "However, many people in deep learning area stick with OpenCV for image loading and processing with some usage of another libraries when it is justified by performance/functionality. This is because OpenCV is in general much faster than the other libraries. Here you can find a couple of benchmarks:\n",
    "- https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading\n",
    "- https://github.com/albumentations-team/albumentations#benchmarking-results\n",
    "\n",
    "To sum up the benchmarks above, there are two most common image formats: PNG and JPEG. If your data is in PNG format - use OpenCV to read it. If it is in JPEG - use libturbojpeg. For image processing, use OpenCV if possible.\n",
    "\n",
    "However, you can find some pitfalls in these image reading/showing/processing operations. Let's discuss them.\n",
    "\n",
    "As you will read the code from other some, you may find out that some of them still use Pillow/something else to read data. You should know, that color image representations in OpenCV and other libraries are different - OpenCV uses \"BGR\" channel order, while others use \"RGB\" one. Good news is that it's easy to convert one representation to another and vice versa.\n",
    "\n",
    "To cope with \"BGR\" <-> \"RGB\" conversion let's recall that image is actually a tensor with the shape `HxWxC`, where `H` is the height of the image, `W` is its width and `C` is the number of channels, in our case - 3. To convert image from \"BGR\" representation to \"RGB\" the only thing we should do is to change channel order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "bgr_image = cv2.imread('../resource/lib/publicdata/images/jersey.jpg')\n",
    "rgb_image = bgr_image[..., ::-1]\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(bgr_image)\n",
    "ax[1].imshow(rgb_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that Matplotlib also uses \"RGB\" channel order, to the right cat image is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">Autograd</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch supports automatic differentiation. The module which implements this is called **Autograd**. It calculates the gradients and keeps track in forward and backward passes. For primitive tensors, you need to enable or disable it using the `requires_grad` flag. But, for advanced tensors, it is enabled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1239, 0.5214, 0.1458, 0.2041, 0.9607],\n",
      "        [0.3995, 0.8332, 0.6348, 0.2301, 0.2707],\n",
      "        [0.6394, 0.5820, 0.9924, 0.1973, 0.0555]], requires_grad=True)\n",
      "tensor([[0.6196, 2.6069, 0.7290, 1.0207, 4.8037],\n",
      "        [1.9977, 4.1662, 3.1738, 1.1506, 1.3536],\n",
      "        [3.1971, 2.9100, 4.9621, 0.9863, 0.2775]], grad_fn=<MulBackward0>)\n",
      "tensor(33.9549, grad_fn=<SumBackward0>)\n",
      "tensor([[5., 5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((3, 5), requires_grad=True)\n",
    "print(a)\n",
    "result = a * 5\n",
    "print(result)\n",
    "\n",
    "# grad can be implicitly created only for scalar outputs\n",
    "# so let's calculate the sum here so that the output becomes a scalar and we can apply a backward pass\n",
    "mean_result = result.sum()\n",
    "print(mean_result)\n",
    "# Calculate Gradient\n",
    "mean_result.backward()\n",
    "# Print gradient of a\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, Pytorch automagically calculated the gradient value for us. It looks to be the correct value - we multiplied an input by 5, so the gradient of this operation equals to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Disabling Autograd for tensors </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to compute gradients for all the variables that are involved in the pipeline. The Pytorch API provides 2 ways to disable autograd.\n",
    "\n",
    "1. `detach` - returns a copy of the tensor with autograd disabled. This copy is built on the same memory as the original tensor, so in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) modifications are **not** allowed.\n",
    "1. `torch.no_grad()` - It is a context manager that allows you to guard a series of operations from autograd without creating new tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2815, 0.8171, 0.8878, 0.7189, 0.3434],\n",
      "        [0.0122, 0.8155, 0.4050, 0.0330, 0.7954],\n",
      "        [0.8516, 0.4766, 0.9781, 0.5248, 0.6398]], requires_grad=True)\n",
      "tensor([[1.4075, 4.0856, 4.4391, 3.5944, 1.7170],\n",
      "        [0.0610, 4.0777, 2.0249, 0.1649, 3.9771],\n",
      "        [4.2582, 2.3829, 4.8904, 2.6240, 3.1992]])\n",
      "tensor([[2.8149, 8.1712, 8.8783, 7.1888, 3.4340],\n",
      "        [0.1220, 8.1555, 4.0497, 0.3298, 7.9541],\n",
      "        [8.5165, 4.7657, 9.7808, 5.2480, 6.3984]], grad_fn=<MulBackward0>)\n",
      "tensor(85.8078, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10.]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((3, 5), requires_grad=True)\n",
    "print(a)\n",
    "detached_a = a.detach()\n",
    "detached_result = detached_a * 5\n",
    "print(detached_result)\n",
    "result = a * 10\n",
    "print(result)\n",
    "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
    "# so let's calculate the sum here\n",
    "mean_result = result.sum()\n",
    "mean_result.backward()\n",
    "print(mean_result)\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3575, 0.7320, 0.1458, 0.0521, 0.6411],\n",
      "        [0.7650, 0.8973, 0.4339, 0.4858, 0.3523],\n",
      "        [0.5275, 0.5736, 0.2598, 0.0412, 0.8134]], requires_grad=True)\n",
      "tensor([[3.5754, 7.3202, 1.4582, 0.5211, 6.4113],\n",
      "        [7.6505, 8.9726, 4.3393, 4.8582, 3.5230],\n",
      "        [5.2747, 5.7357, 2.5975, 0.4121, 8.1336]], grad_fn=<MulBackward0>)\n",
      "tensor(70.7836, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10.]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((3, 5), requires_grad=True)\n",
    "print(a)\n",
    "with torch.no_grad():\n",
    "    detached_result = a * 5\n",
    "result = a * 10\n",
    "print(result)\n",
    "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
    "# so let's calculate the sum here\n",
    "mean_result = result.sum()\n",
    "mean_result.backward()\n",
    "print(mean_result)\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
