https://www.statisticssolutions.com/transforming-data-for-normality/
The above link has image stating when to use which type of transform to make it normal
Correlation:
	1) Collinearity Matrix
	2) Variance Inflation Factor (<=4 No corrlation , >=10 Serious Correlation)
	3) Using L1 and L2 Regularization technique
Counts
Binarization - example listen count
Rounding - By multiplying with 10 or 100
Polynomial
from sklearn.preprocessing import PolynomialFeatures
pf = PolynomialFeatures(degree=2, interaction_only=False,  
                        include_bias=False)
res = pf.fit_transform(atk_def)
res
pd.DataFrame(pf.powers_, columns=['Attack_degree',  
                                  'Defense_degree'])
intr_features = pd.DataFrame(res, columns=['Attack', 'Defense',  
                                           'Attack^2', 
                                           'Attack x Defense',  
                                           'Defense^2'])
intr_features.head(5)

BINNING
FIXED WIDTH BINNING
ADAPTIVE BINNING
quantile_list = [0, .25, .5, .75, 1.]
quantiles = fcc_survey_df['Income'].quantile(quantile_list)
quantiles

Statistical Transformations
	LOG Transform : Log transforms are useful when applied to skewed distributions as they tend to expand 
			the values which fall in the range of lower magnitudes and tend to compress or reduce the 
			values which fall in the range of higher magnitudes. 
			This tends to make the skewed distribution as normal-like as possible. 
	BoxCox Transform:The Box-Cox transform is another popular function belonging to the power transform family of 
			functions. This function has a pre-requisite that the numeric values to be transformed must be 
			positive (similar to what log transform expects). In case they are negative, 
			shifting using a constant value helps. Mathematically, the Box-Cox transform function can be 
			denoted as follows.
scipy.stats.boxcox() API
Below are some common values for lambda:

lambda = -1. is a reciprocal transform.
lambda = -0.5 is a reciprocal square root transform.
lambda = 0.0 is a log transform.
lambda = 0.5 is a square root transform.
lambda = 1.0 is no transform.

income = np.array(fcc_survey_df['Income'])
income_clean = income[~np.isnan(income)]
l, opt_lambda = spstats.boxcox(income_clean)
print('Optimal lambda value:', opt_lambda)

Output
------
Optimal lambda value: 0.117991239456

Now that we have obtained the optimal ? value, let us use the Box-Cox transform for two values of ? 
such that ? = 0 and ? = ?(optimal) and transform the developer Income feature.
fcc_survey_df['Income_boxcox_lambda_0'] = spstats.boxcox(
                                        (1+fcc_survey_df['Income']), 
                                          lmbda=0)
fcc_survey_df['Income_boxcox_lambda_opt'] = spstats.boxcox(
                                            fcc_survey_df['Income'], 
                                              lmbda=opt_lambda)

fcc_survey_df[['ID.x', 'Age', 'Income', 'Income_log', 
               'Income_boxcox_lambda_0',       
               'Income_boxcox_lambda_opt']].iloc[4:9]


------------------------------------

we should add more data points if we suffering from High Variance
We should add more features if we are suffering from High Bias

___________________________________
Cost Sensitive Learning
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight('balanced', np.unique(y), y)