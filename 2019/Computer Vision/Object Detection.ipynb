{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory='C:\\\\Users\\\\Administrator\\\\Documents\\\\GitHub\\\\Code\\\\2019\\\\Udemy\\\\Computer Vision Using Open CV\\\\Master OpenCV\\\\images'\n",
    "directory_home='C:\\\\Users\\\\prudi\\\\Documents\\\\GitHub\\\\Code\\\\2019\\\\Udemy\\\\Computer Vision Using Open CV\\\\Master OpenCV\\\\images'\n",
    "\n",
    "import os\n",
    "os.chdir(directory_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Project # 4 - Finding Waldo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(484, 262)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load input image and convert to grayscale\n",
    "image = cv2.imread('WaldoBeach.jpg')\n",
    "cv2.imshow('Where is Waldo?', image)\n",
    "cv2.waitKey(0)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Load Template image\n",
    "template = cv2.imread('waldo.jpg',0)\n",
    "\n",
    "result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)\n",
    "\n",
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "print(max_loc)\n",
    "\n",
    "#Create Bounding Box\n",
    "top_left = max_loc\n",
    "bottom_right = (top_left[0] + 50, top_left[1] + 50)\n",
    "cv2.rectangle(image, top_left, bottom_right, (0,0,255), 5)\n",
    "\n",
    "cv2.imshow('Where is Waldo?', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Corners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Template Matching\n",
    "\n",
    "There are a variety of methods to perform template matching, but in this case we are using the correlation coefficient which is specified by the flag **cv2.TM_CCOEFF.**\n",
    "\n",
    "So what exactly is the cv2.matchTemplate function doing?\n",
    "Essentially, this function takes a “sliding window” of our waldo query image and slides it across our puzzle image from left to right and top to bottom, one pixel at a time. Then, for each of these locations, we compute the correlation coefficient to determine how “good” or “bad” the match is. \n",
    "\n",
    "Regions with sufficiently high correlation can be considered “matches” for our waldo template.\n",
    "From there, all we need is a call to cv2.minMaxLoc on Line 22 to find where our “good” matches are.\n",
    "That’s really all there is to template matching!\n",
    "\n",
    "http://docs.opencv.org/2.4/modules/imgproc/doc/object_detection.html      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[67.52585    67.52585    67.52585    ...  0.21813272  0.21813272\n",
      "   0.21813272]\n",
      " [67.52585    67.52585    67.52585    ...  0.23221451  0.23221451\n",
      "   0.23221451]\n",
      " [67.52585    67.52585    67.52585    ...  0.31414932  0.31414932\n",
      "   0.31414932]\n",
      " ...\n",
      " [ 0.13541663  0.13541663  0.13541663 ...  0.31549966  0.31549966\n",
      "   0.31549966]\n",
      " [ 0.13541663  0.13541663  0.13541663 ...  0.31549966  0.31549966\n",
      "   0.31549966]\n",
      " [ 0.13541663  0.13541663  0.13541663 ...  0.31549966  0.31549966\n",
      "   0.31549966]]\n",
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image then grayscale\n",
    "image = cv2.imread('chess.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# The cornerHarris function requires the array datatype to be float32\n",
    "gray = np.float32(gray)\n",
    "\n",
    "harris_corners = cv2.cornerHarris(gray, 3, 3, 0.05)\n",
    "\n",
    "#We use dilation of the corner points to enlarge them\\\n",
    "kernel = np.ones((7,7),np.uint8)\n",
    "harris_corners = cv2.dilate(harris_corners, kernel, iterations = 2)\n",
    "print(harris_corners)\n",
    "print(harris_corners > 0.025 * harris_corners.max())\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "image[harris_corners > 0.025 * harris_corners.max()] = [255, 127, 127]\n",
    "\n",
    "cv2.imshow('Harris Corners', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harris Corner Detection is an algorithm developed in 1998 for corner detection  (http://www.bmva.org/bmvc/1988/avc-88-023.pdf) and works fairly well.\n",
    "\n",
    "**cv2.cornerHarris**(input image, block size, ksize, k)\n",
    "- Input image - should be grayscale and float32 type.\n",
    "- blockSize - the size of neighborhood considered for corner detection\n",
    "- ksize - aperture parameter of Sobel derivative used.\n",
    "- k - harris detector free parameter in the equation\n",
    "- **Output** – array of corner locations (x,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Corner Detection using - Good Features to Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('chess.jpg')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# We specific the top 50 corners\n",
    "corners = cv2.goodFeaturesToTrack(gray, 100, 0.01, 15)\n",
    "\n",
    "for corner in corners:\n",
    "    x, y = corner[0]\n",
    "    x = int(x)\n",
    "    y = int(y)\n",
    "    cv2.rectangle(img,(x-10,y-10),(x+10,y+10),(0,255,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Corners Found\", img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cv2.goodFeaturesToTrack**(input image, maxCorners, qualityLevel, minDistance)\n",
    "\n",
    "- Input Image - 8-bit or floating-point 32-bit, single-channel image.\n",
    "- maxCorners – Maximum number of corners to return. If there are more corners than are found, the strongest of them is returned.\n",
    "- qualityLevel – Parameter characterizing the minimal accepted quality of image corners. The parameter value is multiplied by the best corner quality measure (smallest eigenvalue). The corners with the quality measure less than the product are rejected. For example, if the best corner has the quality measure = 1500, and the  qualityLevel=0.01 , then all the corners with the quality - - measure less than 15 are rejected.\n",
    "- minDistance – Minimum possible Euclidean distance between the returned corners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keypoints Detected:  1893\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('input.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Create SIFT Feature Detector object\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "#Detect key points\n",
    "\n",
    "keypoints = sift.detect(gray, None)\n",
    "print(\"Number of keypoints Detected: \", len(keypoints))\n",
    "\n",
    "# Draw rich key points on input image\n",
    "blank = np.zeros((1,1)) \n",
    "image = cv2.drawKeypoints(image, keypoints, blank, (0,255,255),flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('Feature Method - SIFT', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "sift = cv2.xfeatures2d.SIFT_create()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
